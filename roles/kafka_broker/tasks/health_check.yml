---
- name: Get Topics with UnderReplicatedPartitions
  shell: |
    {{ binary_base_path }}/bin/kafka-topics --bootstrap-server {{ hostvars[inventory_hostname]|confluent.platform.resolve_hostname }}:{{kafka_broker_listeners[kafka_broker_inter_broker_listener_name]['port']}} \
      --describe --under-replicated-partitions --command-config {{kafka_broker.client_config_file}}
  environment:
    KAFKA_OPTS: "-Xlog:all=error -XX:+IgnoreUnrecognizedVMOptions"
  register: urp_topics
  # stdout_lines will have topics with URPs and stderr has WARN and ERROR level logs
  until: urp_topics.stdout_lines|length == 0 and 'ERROR' not in urp_topics.stderr
  retries: 15
  delay: 5
  ignore_errors: true
  changed_when: false
  check_mode: false
  when:
    - not rbac_enabled|bool
    - inventory_hostname in groups.kafka_broker

- name: Get Topics with UnderReplicatedPartitions with Secrets Protection enabled
  shell: |
    {{ binary_base_path }}/bin/kafka-topics --bootstrap-server {{inventory_hostname}}:{{kafka_broker_listeners[kafka_broker_inter_broker_listener_name]['port']}} \
      --describe --under-replicated-partitions --command-config {{kafka_broker.client_config_file}}
  environment:
    CONFLUENT_SECURITY_MASTER_KEY: "{{ secrets_protection_masterkey }}"
    KAFKA_OPTS: "-Xlog:all=error -XX:+IgnoreUnrecognizedVMOptions"
  register: urp_topics_secrets_protection
  # stdout_lines will have topics with URPs and stderr has WARN and ERROR level logs
  until: urp_topics_secrets_protection.stdout_lines|length == 0 and 'ERROR' not in urp_topics_secrets_protection.stderr
  retries: 15
  delay: 5
  ignore_errors: true
  changed_when: false
  check_mode: false
  when:
    - kafka_broker_client_secrets_protection_enabled|bool
    - rbac_enabled|bool
    - inventory_hostname in groups.kafka_broker

# health check for kafka controller
- name: Add controller listener in client properties
  shell: |
    echo confluent.use.controller.listener=true >> {{kafka_broker.client_config_file}}
  ignore_errors: true
  changed_when: false
  check_mode: false
  when:
    - kraft_enabled|bool and inventory_hostname in groups.kafka_controller

- name: Check kafka-metadata-quorum
  shell: |
    {{ binary_base_path }}/bin/kafka-metadata-quorum --bootstrap-server {{inventory_hostname}}:9093 \
      --command-config {{kafka_broker.client_config_file}} describe --replication
  environment:
    KAFKA_OPTS: "-Xlog:all=error -XX:+IgnoreUnrecognizedVMOptions"
  ignore_errors: false
  changed_when: false
  check_mode: false
  when:
    - kraft_enabled|bool and inventory_hostname in groups.kafka_controller

- name: Register LogEndOffset
  shell: |
    {{ binary_base_path }}/bin/kafka-metadata-quorum --bootstrap-server {{inventory_hostname}}:9093 \
      --command-config {{kafka_broker.client_config_file}} describe --replication | awk '{print $2}'
  environment:
    KAFKA_OPTS: "-Xlog:all=error -XX:+IgnoreUnrecognizedVMOptions"
  register: LEO
  ignore_errors: false
  changed_when: false
  check_mode: false
  when:
    - kraft_enabled|bool and inventory_hostname in groups.kafka_controller

- name: Check LogEndOffset values
  assert:
    that:
      - "{{ item|int > 0 and LEO.stdout_lines[1:]|max|int - item|int < 1000 }}"
    fail_msg: "UnreachableQuorumMember or Found at least one quorum voter with an offset {{ item }}, while the primary controller was at offset {{ LEO.stdout_lines[1:]|max}}
               The max allowed offset lag is 1000"
  loop: "{{ LEO.stdout_lines[1:] }}"
  ignore_errors: false
  changed_when: false
  check_mode: false
  when:
    - kraft_enabled|bool and inventory_hostname in groups.kafka_controller

- name: Wait for Metadata Service to start
  uri:
    url: "{{mds_http_protocol}}://{{ mds_advertised_listener_hostname | default(hostvars[inventory_hostname]|confluent.platform.resolve_hostname, True) }}:{{mds_port}}/security/1.0/authenticate"
    validate_certs: false
    return_content: true
    status_code: 200
    url_username: "{{mds_health_check_user}}"
    url_password: "{{mds_health_check_password}}"
    force_basic_auth: true
  register: mds_result
  until: mds_result.status == 200
  retries: 30
  delay: 5
  ignore_errors: true
  when:
    - rbac_enabled|bool and not external_mds_enabled|bool
    # Skip if any previous check failed
    - not urp_topics.failed|default(False) and not urp_topics_secrets_protection.failed|default(False)
    - not fips_enabled|bool

- name: Wait for Embedded Rest Proxy to start
  uri:
    url: "{{kafka_broker_erp_clusters_url}}"
    validate_certs: false
    return_content: true
    status_code: 200
    url_username: "{{kafka_broker_rest_health_check_user}}"
    url_password: "{{kafka_broker_rest_health_check_password}}"
    force_basic_auth: true
  register: erp_result
  until: erp_result.status == 200
  retries: 25
  delay: 5
  ignore_errors: true
  when:
    - kafka_broker_rest_proxy_enabled|bool
    # Skip if any previous checks failed
    - not urp_topics.failed|default(False) and not urp_topics_secrets_protection.failed|default(False) and not mds_result.failed|default(False)
    - not fips_enabled|bool

- name: Fetch Log Files and Error out
  block:
    - name: Fetch Files for Debugging Failure
      # Cannot use include_role in Ansible Handlers: https://github.com/ansible/ansible/issues/20493
      include_tasks: ../../common/tasks/fetch_logs.yml
      vars:
        service_name: "{{kafka_broker_service_name}}"
        config_file: "{{kafka_broker.config_file}}"
        log_dir: "{{kafka_broker_service_environment_overrides.LOG_DIR}}"
        user: "{{kafka_broker_user}}"
        group: "{{kafka_broker_group}}"

    - name: Fail Provisioning
      fail:
        msg: Health checks failed. Review exported files.
  # When only one health check runs, only one will have a 'failed' field. For skipped checks, defaulting 'failed' to False
  when: urp_topics.failed|default(False) or urp_topics_secrets_protection.failed|default(False) or mds_result.failed|default(False) or erp_result.failed|default(False)
