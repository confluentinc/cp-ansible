---
- include_role:
    name: common
  when: not common_role_completed|bool
  tags: common

- name: Gather OS Facts
  setup:
    # Only gathers items in list, filters out the rest
    filter: "{{item}}"
    gather_subset:
      - '!all'
  loop:
    - ansible_os_family
    - ansible_fqdn
    - ansible_distribution

- name: Validate cert principals defined in inventory for RBAC mTLS setup
  debug:
    msg: "Warning: Please add list of broker and controller certificate principals in rbac_super_users variable"
  when:
    - rbac_enabled|bool
    - ssl_mutual_auth_enabled|bool
    - rbac_super_users | length == 0

# datadir is no longer supported method to provide data directories for kafka brokers
- name: Assert that datadir is not present in the inventory
  assert:
    that: >
      'kafka_broker' not in hostvars[item] or
      hostvars[item].kafka_broker == None or
      'datadir' not in hostvars[item].kafka_broker
    fail_msg: |
      "The 'datadir' configuration property is now obsolete for configuration of kafka broker data directories."
      "Please use 'log.dirs' under kafka_broker_custom_properties with comma separated directories."
  loop: "{{ groups['kafka_broker'] }}"

# To avoid a catastrophic user error where root directory permissions get recursively changed
- name: Assert log.dirs Property not Misconfigured
  assert:
    that:
      - kafka_broker_final_properties['log.dirs'].split(',')[0] != "/"
    fail_msg: "If you have log.dirs in kafka_broker_custom_properties, make sure it's comma separated directories."

- name: Stop Service and Remove Packages on Version Change
  include_role:
    name: common
    tasks_from: remove_packages.yml
  vars:
    service_name: "{{ kafka_broker_service_name }}"
  when: installation_method == "package"
  tags:
    - package
    - cp_package


- name: Validate Kafka Broker Jolokia Access Control Configuration
  include_tasks: ../common/tasks/component_validations.yml
  vars:
    component_name: "Kafka Broker"
    component_jolokia_enabled: "{{ kafka_broker_jolokia_enabled }}"
    component_jolokia_access_control_enabled: "{{ kafka_broker_jolokia_access_control_enabled }}"
    component_jolokia_access_control_custom_file_enabled: "{{ kafka_broker_jolokia_access_control_custom_file_enabled }}"
    component_jolokia_access_control_file_src_path: "{{ kafka_broker_jolokia_access_control_file_src_path }}"
  tags:
    - validate
    - validate_jolokia

- name: Install the Kafka Broker Packages
  yum:
    name: "{{ kafka_broker_packages | product([confluent_package_redhat_suffix]) | map('join') | list }}"
    state: latest
  register: install_kafka_package_result
  until: install_kafka_package_result is success  or ansible_check_mode
  retries: 5
  delay: 90
  when:
    - ansible_os_family == "RedHat"
    - installation_method == "package"
  ignore_errors: "{{ ansible_check_mode }}"
  tags:
    - package
    - cp_package
  notify: restart kafka

- name: Install the Kafka Broker Packages
  apt:
    name: "{{ kafka_broker_packages | product([confluent_package_debian_suffix]) | map('join') | list }}"
  register: install_kafka_package_result
  until: install_kafka_package_result is success  or ansible_check_mode
  retries: 5
  delay: 90
  when:
    - ansible_os_family == "Debian"
    - installation_method == "package"
  ignore_errors: "{{ ansible_check_mode }}"
  tags:
    - package
    - cp_package
  notify: restart kafka

- name: Kafka Broker group
  group:
    name: "{{kafka_broker_group}}"
  tags:
    - privileged

- name: Check if Kafka Broker User Exists
  # Create user task must be skipped for non-linux users
  getent:
    database: passwd
    key: "{{kafka_broker_user}}"
  failed_when: false

- name: Create Kafka Broker user
  user:
    name: "{{kafka_broker_user}}"
    comment: Confluent Kafka
    system: true
    shell: "{{user_login_shell}}"
    group: "{{kafka_broker_group}}"
  when: (getent_passwd|default({}))[kafka_broker_user] is not defined
  tags:
    - privileged

# Archive File deployments need to create SystemD service units
# Copy the tarball's systemd service to the system
- name: Copy Kafka Broker Service from archive file to system
  copy:
    src: "{{binary_base_path}}/lib/systemd/system/{{kafka_broker.systemd_file|basename}}"
    remote_src: true
    dest: "{{kafka_broker.systemd_file}}"
    mode: '644'
    force: true
  when: installation_method == "archive"
  tags:
    - systemd
    - privileged

- include_role:
    name: ssl
  vars:
    truststore_storepass: "{{kafka_broker_truststore_storepass}}"
    truststore_path: "{{kafka_broker_pkcs12_truststore_path}}"
    keystore_path: "{{kafka_broker_pkcs12_keystore_path}}"
    keystore_storepass: "{{kafka_broker_keystore_storepass}}"
    keystore_keypass: "{{kafka_broker_keystore_keypass}}"
    service_name: kafka_broker
    user: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
    hostnames: "{{ ([inventory_hostname, ansible_fqdn, ansible_host, ansible_ssh_host|default(omit), hostname|default(omit)] + kafka_broker_listeners | confluent.platform.get_hostnames(inventory_hostname)) | unique }}"
    ca_cert_path: "{{kafka_broker_ca_cert_path}}"
    cert_path: "{{kafka_broker_cert_path}}"
    key_path: "{{kafka_broker_key_path}}"
    export_certs: "{{kafka_broker_export_certs}}"
    create_bouncy_castle_keystore: "{{fips_enabled}}"
    bcfks_truststore_path: "{{kafka_broker_bcfks_truststore_path}}"
    bcfks_keystore_path: "{{kafka_broker_bcfks_keystore_path}}"
  when: >
    kafka_controller_ssl_enabled|bool or
    kafka_broker_listeners | confluent.platform.ssl_required(ssl_enabled) or
    kafka_broker_rest_ssl_enabled|bool or
    mds_broker_listener.ssl_enabled|bool or
    mds_tls_enabled|bool or
    ( kafka_broker_schema_validation_enabled and 'schema_registry' in groups and schema_registry_ssl_enabled )
  tags: ssl

- include_tasks: rbac.yml
  when: rbac_enabled|bool

- name: Import IDP certificate to Truststore for OAuth
  include_role:
    name: common
    tasks_from: idp_certs.yml
  vars:
    idp_cert_path: "{{ oauth_idp_cert_path }}"
    idp_cert_dest: /tmp/oauth_idp_cert.pem
    alias: "oauth_cert"
    truststore_storepass: "{{kafka_broker_truststore_storepass}}"
    truststore_path: "{{kafka_broker_pkcs12_truststore_path}}"
    bcfks_truststore_path: "{{kafka_broker_bcfks_truststore_path}}"
    create_bouncy_castle_keystore: "{{fips_enabled}}"
    restart_handler_name: "restart kafka" # adding cert to Truststore requires a restart of the service to get loaded
    # cant directly call notify handler from include_role task.
  when:
    - oauth_enabled|bool or schema_registry_oauth_enabled|bool or kafka_rest_oauth_enabled|bool or kafka_connect_oauth_enabled|bool or kafka_connect_replicator_oauth_enabled|bool
    - oauth_idp_cert_path != ""

- name: Import IDP certificate to MDS Truststore for C3 SSO
  include_role:
    name: common
    tasks_from: idp_certs.yml
  vars:
    idp_cert_path: "{{ sso_idp_cert_path }}"
    idp_cert_dest: /tmp/sso_idp_cert.pem
    alias: "sso_cert"
    truststore_storepass: "{{kafka_broker_truststore_storepass}}"
    truststore_path: "{{kafka_broker_pkcs12_truststore_path}}"
    bcfks_truststore_path: "{{kafka_broker_bcfks_truststore_path}}"
    create_bouncy_castle_keystore: "{{fips_enabled}}"
    restart_handler_name: "restart kafka" # adding cert to Truststore requires a restart of the service to get loaded
    # cant directly call notify handler from include_role task.
  when:
    - sso_mode != 'none'
    - sso_idp_cert_path != ""
    - not external_mds_enabled|bool

- name: Configure Kerberos
  include_role:
    name: kerberos
  vars:
    kerberos_group: "{{kafka_broker_group}}"
    kerberos_user: "{{kafka_broker_user}}"
    kerberos_keytab_path: "{{kafka_broker_kerberos_keytab_path}}"
    kerberos_keytab_destination_path: "{{kafka_broker_keytab_path}}"
    kerberos_handler: "restart kafka"
  when: "'GSSAPI' in kafka_sasl_enabled_mechanisms or mds_broker_listener.sasl_protocol =='kerberos'"

- name: Copy Custom Kafka Files
  include_role:
    name: common
    tasks_from: copy_files.yml
  vars:
    copy_files: "{{kafka_broker_copy_files}}"
    user: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
  when: kafka_broker_copy_files | length > 0
  tags:
    - configuration

- name: Copy Custom Files From All Components List
  include_role:
    name: common
    tasks_from: copy_files.yml
  vars:
    copy_files: "{{all_components_copy_files}}"
    user: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
  when: all_components_copy_files | length > 0
  tags:
    - configuration

- name: Set Permissions on /var/lib/kafka
  file:
    path: /var/lib/kafka/
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
    state: directory
    mode: '750'
  tags:
    - privileged
    - filesystem

- name: Create Kafka Broker Config directory
  file:
    path: "{{ kafka_broker.config_file | dirname }}"
    state: directory
    mode: '750'
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
  tags:
    - filesystem

- name: Copy SuperUser client assertion files
  include_role:
    name: common
    tasks_from: copy_client_assertion_files.yml
  vars:
    user: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
    client_assertion_files:
      - src: "{{ oauth_superuser_client_assertion_private_key_file }}"
        dest: "{{ oauth_superuser_client_assertion_private_key_file_dest_path }}"
      - src: "{{ oauth_superuser_client_assertion_template_file }}"
        dest: "{{ oauth_superuser_client_assertion_template_file_dest_path }}"
  tags:
    - configuration

- name: Copy broker client assertion files
  include_role:
    name: common
    tasks_from: copy_client_assertion_files.yml
  vars:
    user: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
    client_assertion_files:
      - src: "{{ kafka_broker_oauth_client_assertion_private_key_file }}"
        dest: "{{ kafka_broker_oauth_client_assertion_private_key_file_dest_path }}"
      - src: "{{ kafka_broker_oauth_client_assertion_template_file }}"
        dest: "{{ kafka_broker_oauth_client_assertion_template_file_dest_path }}"
  tags:
    - configuration

- name: Copy schema registry client assertion files
  include_role:
    name: common
    tasks_from: copy_client_assertion_files.yml
  vars:
    user: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
    client_assertion_files:
      - src: "{{ schema_registry_oauth_client_assertion_private_key_file }}"
        dest: "{{ kafka_broker_sr_oauth_client_assertion_private_key_file_dest_path }}"
      - src: "{{ schema_registry_oauth_client_assertion_template_file }}"
        dest: "{{ kafka_broker_sr_oauth_client_assertion_template_file_dest_path }}"
  tags:
    - configuration

- name: Get kafka broker secrets protection file status
  stat:
    path: "{{ kafka_broker_secrets_protection_file }}"
  register: kafka_broker_secrets_protection_file_status

- name: Create Kafka Broker Config
  template:
    src: server.properties.j2
    dest: "{{kafka_broker.config_file}}"
    mode: '640'
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
  notify: restart kafka
  diff: "{{ not mask_sensitive_diff|bool }}"
  when: not kafka_broker_secrets_protection_enabled|bool or not kafka_broker_secrets_protection_file_status.stat.exists
  tags:
    - configuration

- name: Get kafka broker client secrets protection file status
  stat:
    path: "{{ kafka_broker_client_secrets_protection_file }}"
  register: kafka_broker_client_secrets_protection_file_status

- name: Create Kafka Broker Client Config
  template:
    src: client.properties.j2
    dest: "{{kafka_broker.client_config_file}}"
    mode: '640'
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
  diff: "{{ not mask_sensitive_diff|bool }}"
  when: not kafka_broker_client_secrets_protection_enabled|bool or not kafka_broker_client_secrets_protection_file_status.stat.exists
  tags:
    - configuration

- name: Include Kraft Cluster Data
  include_tasks: get_meta_properties.yml

- name: Create Logs Directory
  file:
    path: "{{kafka_broker_log_dir}}"
    state: directory
    group: "{{kafka_broker_group}}"
    owner: "{{kafka_broker_user}}"
    mode: '770'
  tags:
    - filesystem
    - log

- name: Set Permissions on Data Dirs
  file:
    path: "{{item}}"
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
    state: directory
    mode: '750'
  with_items: "{{ kafka_broker_final_properties['log.dirs'].split(',') }}"
  tags:
    - filesystem

- name: Check Green/Brownfield setup
  set_fact:
    is_brownfield_setup: "{{ broker_service_state == 'running' }}"
  vars:
    broker_service_state: "{{ ansible_facts.services[kafka_broker_service_name + '.service'].state | default('unknown') }}"

- name: Set Permissions on Data Dir files  # noqa no-free-form
  # Have to use command + chown instead of the Ansible file module here as it seems
  # like the file module can't handle files modified/deleted while running. This task
  # can fail on a very active cluster. See https://github.com/confluentinc/cp-ansible/pull/903 for details.
  command: chown -R {{ kafka_broker_user }}:{{ kafka_broker_group }} {{ item }}
  changed_when: false
  with_items: "{{ kafka_broker_final_properties['log.dirs'].split(',') }}"
  when: not is_brownfield_setup # Skips this task during upgrades as the user/group is already set in the initial setup run. This will avoid the race condition caused by sudden deletion of .tmp files in data dir.
  tags:
    - filesystem

- name: Create logredactor rule file directory
  file:
    path: "{{ logredactor_rule_path | dirname }}"
    state: directory
    mode: '755'
  when: (kafka_broker_custom_log4j2|bool) and (logredactor_enabled|bool) and (logredactor_rule_url == '')
  tags:
    - log

- name: Copy logredactor rule file from control node to component node
  copy:
    src: "{{ logredactor_rule_path_local }}"
    dest: "{{ logredactor_rule_path }}"
    mode: '644'
  when: (kafka_broker_custom_log4j2|bool) and (logredactor_enabled|bool) and (logredactor_rule_url == '')
  tags:
    - log

- name: Check if Kafka log4j file exists
  stat:
    path: "{{ kafka_broker.log4j_file }}"
  register: log4j_file_stat
  tags:
    - log

- name: Backup Kafka log4j file if it exists
  copy:
    src: "{{ kafka_broker.log4j_file }}"
    dest: "{{ kafka_broker.log4j_file }}.bak"
    remote_src: true
    mode: '640'
  when: log4j_file_stat.stat.exists
  tags:
    - log

- name: Update Kafka log4j Config for Log Cleanup
  confluent.platform.update_log4j2:
    path: "{{kafka_broker.log4j_file}}"
    max: "{{kafka_broker_max_log_files}}"
    size: "{{kafka_broker_log_file_size}}"
    root_level: "{{kafka_broker_log4j2_root_logger_level}}"
    root_appenders: "{{ kafka_broker_log4j2_root_appenders }}"
    add_redactor: "{{ logredactor_enabled|bool }}"
    redactor_refs: "{{ kafka_broker_logredactor_logger_specs_list | map(attribute='appenderRefs') | list }}"
    redactor_rules: "{% if logredactor_rule_url == '' %}{{logredactor_rule_path}}{% else %}{{logredactor_rule_url}}{% endif %}"
    redactor_policy_refresh_interval: "{{ logredactor_policy_refresh_interval }}"
    redactor_logger_names: "{{ kafka_broker_logredactor_logger_specs_list | map(attribute='logger_name') | list }}"
  notify: restart kafka
  when: kafka_broker_custom_log4j2|bool
  tags:
    - configuration
    - log

- name: Set Permissions on Log4j Conf
  file:
    path: "{{kafka_broker.log4j_file}}"
    group: "{{kafka_broker_group}}"
    owner: "{{kafka_broker_user}}"
    mode: '640'
  tags:
    - filesystem
    - log

- name: Restart kafka broker
  ansible.builtin.debug:
    msg: "restarting kafka broker"
  notify: restart kafka
  when: (kafka_broker_custom_log4j2|bool) and (logredactor_enabled|bool) and (not kafka_broker_skip_restarts|bool)
  tags:
    - log

- name: Create Kafka Broker Jolokia Config
  template:
    src: kafka_jolokia.properties.j2
    dest: "{{kafka_broker_jolokia_config}}"
    mode: '640'
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
  when: kafka_broker_jolokia_enabled|bool
  notify: restart kafka
  diff: "{{ not mask_sensitive_diff|bool }}"
  tags:
    - configuration


- name: Copy Kafka Broker Jolokia Access Control File
  copy:
    src: "{{ kafka_broker_jolokia_access_control_file_src_path }}"
    dest: "{{ kafka_broker_jolokia_access_control_file_dest_path }}"
    mode: '640'
    owner: "{{ kafka_broker_user }}"
    group: "{{ kafka_broker_group }}"
  when:
    - kafka_broker_jolokia_enabled|bool
    - kafka_broker_jolokia_access_control_enabled|bool
  notify: restart kafka
  tags:
    - configuration

- name: Create Kafka Broker Jaas Config
  template:
    src: kafka_server_jaas.conf.j2
    dest: "{{kafka_broker.jaas_file}}"
    mode: '640'
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
  when: "'GSSAPI' in kafka_sasl_enabled_mechanisms or
          (kafka_broker_rest_proxy_enabled and (not rbac_enabled or (rbac_enabled and external_mds_enabled)) and kafka_broker_rest_proxy_authentication_type == 'basic')"
  notify: restart kafka
  tags:
    - configuration

- name: Create Kafka Broker Password File
  template:
    src: password.properties.j2
    dest: "{{kafka_broker.rest_proxy_password_file}}"
    mode: '640'
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
  when: kafka_broker_rest_proxy_enabled and (not rbac_enabled or (rbac_enabled and external_mds_enabled)) and kafka_broker_rest_proxy_authentication_type == 'basic'
  notify: restart kafka
  diff: "{{ not mask_sensitive_diff|bool }}"

- name: Deploy JMX Exporter Config File
  template:
    src: "{{kafka_broker_jmxexporter_config_source_path}}"
    dest: "{{kafka_broker_jmxexporter_config_path}}"
    mode: '640'
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
  when: kafka_broker_jmxexporter_enabled|bool
  tags:
    - configuration

- name: Create Service Override Directory
  file:
    path: "{{kafka_broker.systemd_override | dirname }}"
    owner: "{{kafka_broker_user}}"
    group: "{{kafka_broker_group}}"
    state: directory
    mode: '750'
  tags:
    - systemd
    - privileged

- name: Write Service Overrides
  template:
    src: override.conf.j2
    dest: "{{ kafka_broker.systemd_override }}"
    mode: '640'
    owner: root
    group: root
  notify: restart kafka
  diff: "{{ not mask_sensitive_diff|bool }}"
  tags:
    - systemd
    - privileged

- name: Create sysctl directory on Debian distributions
  file:
    path: /usr/lib/sysctl.d
    state: directory
    mode: '755'
  when: ansible_distribution == "Debian"
  tags:
    - sysctl
    - privileged

- name: Tune virtual memory settings
  ansible.posix.sysctl:
    name: "{{ item.key }}"
    value: "{{ item.value }}"
    state: present
    sysctl_file: "{{ kafka_broker_sysctl_file }}"
    reload: true
  with_dict: "{{ kafka_broker_sysctl }}"
  tags:
    - sysctl
    - privileged

- name: Certs were Updated - Trigger Restart
  command: /bin/true
  notify: restart kafka
  changed_when: false
  when: certs_updated|bool

# We need to start MDS here to proceed with secrets protection
- meta: flush_handlers

# Starting confluent cli v2, secrets protection works only when rbac
- name: Encrypt secrets
  include_tasks: secrets_protection.yml
  when:
    - kafka_broker_secrets_protection_enabled|bool or kafka_broker_client_secrets_protection_enabled|bool
    - rbac_enabled|bool or confluent_cli_version is version('3.0.0', '>=')

- name: Encrypt Controller secrets
  include_role:
    name: kafka_controller
    tasks_from: secrets_protection.yml
  vars:
    controller_host: "{{ item }}"
  loop: "{{ groups['kafka_controller'] }}"
  args:
    apply:
      delegate_to: "{{item}}"
  when:
    - kafka_controller_secrets_protection_enabled|bool or kafka_controller_client_secrets_protection_enabled|bool
    - rbac_enabled|bool
    - (kafka_controller_ssl_mutual_auth_enabled|bool and rbac_super_users | length == 0)
      or ( not external_mds_enabled|bool and confluent_cli_version is version('3.0.0', '<'))
  run_once: true

- meta: flush_handlers

- name: Kafka Started
  systemd:
    name: "{{kafka_broker_service_name}}"
    enabled: true
    state: started
  tags:
    - systemd

- name: Wait for Broker health checks to complete
  include_tasks: health_check.yml
  when:
    - kafka_broker_health_checks_enabled|bool
    - not ansible_check_mode
  tags: health_check

- name: Create SCRAM Users with KRaft
  shell: |
    {{ binary_base_path }}/bin/kafka-configs \
      --bootstrap-server {{ hostvars[inventory_hostname]|confluent.platform.resolve_hostname }}:{{kafka_broker_listeners[kafka_broker_inter_broker_listener_name]['port']}} \
      --command-config {{ kafka_broker.client_config_file }} \
      --alter --add-config 'SCRAM-SHA-512=[password={{ item.value['password'] }}]' \
      --entity-type users --entity-name {{ item.value['principal'] }}
  loop: "{{ sasl_scram_users_final|dict2items }}"
  run_once: true
  changed_when: true
  when:
    - "'SCRAM-SHA-512' in kafka_sasl_enabled_mechanisms"
    - not ( kafka_broker_client_secrets_protection_enabled|bool )
  no_log: "{{ mask_secrets|bool }}"

- name: Create SCRAM 256 Users with KRaft
  shell: |
    {{ binary_base_path }}/bin/kafka-configs \
      --bootstrap-server {{ hostvars[inventory_hostname]|confluent.platform.resolve_hostname }}:{{kafka_broker_listeners[kafka_broker_inter_broker_listener_name]['port']}} \
      --command-config {{ kafka_broker.client_config_file }} \
      --alter --add-config 'SCRAM-SHA-256=[password={{ item.value['password'] }}]' \
      --entity-type users --entity-name {{ item.value['principal'] }}
  loop: "{{ sasl_scram256_users_final|dict2items }}"
  run_once: true
  changed_when: true
  when:
    - "'SCRAM-SHA-256' in kafka_sasl_enabled_mechanisms"
    - not ( kafka_broker_client_secrets_protection_enabled|bool )
  no_log: "{{ mask_secrets|bool }}"

- name: Create SCRAM Users with KRaft with Secrets Protection enabled
  shell: |
    {{ binary_base_path }}/bin/kafka-configs \
      --bootstrap-server {{ hostvars[inventory_hostname]|confluent.platform.resolve_hostname }}:{{kafka_broker_listeners[kafka_broker_inter_broker_listener_name]['port']}} \
      --command-config {{ kafka_broker.client_config_file }} \
      --alter --add-config 'SCRAM-SHA-512=[password={{ item.value['password'] }}]' \
      --entity-type users --entity-name {{ item.value['principal'] }}
  environment:
    CONFLUENT_SECURITY_MASTER_KEY: "{{ secrets_protection_masterkey }}"
  loop: "{{ sasl_scram_users_final|dict2items }}"
  run_once: true
  changed_when: true
  when:
    - "'SCRAM-SHA-512' in kafka_sasl_enabled_mechanisms"
    - kafka_broker_client_secrets_protection_enabled|bool
  no_log: "{{ mask_secrets|bool }}"

- name: Create SCRAM 256 Users with KRaft with Secrets Protection enabled
  shell: |
    {{ binary_base_path }}/bin/kafka-configs \
      --bootstrap-server {{ hostvars[inventory_hostname]|confluent.platform.resolve_hostname }}:{{kafka_broker_listeners[kafka_broker_inter_broker_listener_name]['port']}} \
      --command-config {{ kafka_broker.client_config_file }} \
      --alter --add-config 'SCRAM-SHA-256=[password={{ item.value['password'] }}]' \
      --entity-type users --entity-name {{ item.value['principal'] }}
  environment:
    CONFLUENT_SECURITY_MASTER_KEY: "{{ secrets_protection_masterkey }}"
  loop: "{{ sasl_scram256_users_final|dict2items }}"
  run_once: true
  changed_when: true
  when:
    - "'SCRAM-SHA-256' in kafka_sasl_enabled_mechanisms"
    - kafka_broker_client_secrets_protection_enabled|bool
  no_log: "{{ mask_secrets|bool }}"

- name: Register Cluster
  include_tasks: register_cluster.yml
  when: kafka_broker_cluster_name|length > 0 and rbac_enabled|bool

- name: Create RBAC Rolebindings
  include_tasks: rbac_rolebindings.yml
  when: kafka_broker_additional_system_admins|length > 0

- name: Delete temporary keys/certs when keystore and trustore is provided
  file:
    path: "{{item}}"
    state: absent
  loop:
    - "{{kafka_broker_ca_cert_path}}"
    - "{{kafka_broker_cert_path}}"
    - "{{kafka_broker_key_path}}"
  when:
    - (ssl_provided_keystore_and_truststore | bool)
    - not (rbac_enabled|bool and mds_ssl_client_authentication in ['required', 'requested'])
    # not deleting the certs becase they are needed to be present here so other components can use delegate to make broker fetch a super user token for assigning role bindings
