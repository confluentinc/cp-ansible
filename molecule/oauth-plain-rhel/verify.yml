---
### Validates that custom log4j appenders are present on each component.
### Validates that Service Description has been overridden.
### Validates that SASL Plaintext protocol is set across components.
### Validates that Connectors are present on Kafka Connect.

- name: Verify - kafka_controller
  hosts: kafka_controller
  gather_facts: false
  tasks:
    - shell: grep "apache.kafka.sasl.oauthbearer.allowed.urls=https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/token,https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/certs" /etc/systemd/system/confluent-kcontroller.service.d/override.conf
      changed_when: false

    - import_role:
        name: variables
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/controller/server.properties
        property: controller.quorum.voters
        expected_value: "{{ kafka_controller_quorum_voters }}"

    - name: Check Updated Service Description
      shell: systemctl status confluent-kcontroller
      register: systemctl_status
      failed_when: "'Custom Controller description' not in systemctl_status.stdout_lines[0]"
      changed_when: false

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/controller/server.properties
        property: listener.name.controller.sasl.enabled.mechanisms
        expected_value: PLAIN

- name: Verify - kafka_broker
  hosts: kafka_broker
  gather_facts: false
  tasks:
    - shell: grep "apache.kafka.sasl.oauthbearer.allowed.urls=https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/token,https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/certs" /etc/systemd/system/confluent-server.service.d/override.conf
      changed_when: false

    - set_fact:
        kraft_mode: "{{ ('kafka_controller' in groups.keys() and groups['kafka_controller'] | length > 0) }}"

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka/server.properties
        property: listener.name.internal.sasl.enabled.mechanisms
        expected_value: OAUTHBEARER

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka/server.properties
        property: transaction.state.log.min.isr
        expected_value: "2"

    - name: Check Updated Service Description
      shell: systemctl status confluent-server
      register: systemctl_status
      failed_when: "'Custom Kafka description' not in systemctl_status.stdout_lines[0]"
      changed_when: false

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka/client.properties
        property: default.api.timeout.ms
        expected_value: "40000"

- name: Verify - schema_registry
  hosts: schema_registry
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/schema-registry/schema-registry.properties
        property: kafkastore.security.protocol
        expected_value: SASL_SSL

    - name: Check Original Service Description
      shell: systemctl status confluent-schema-registry
      register: systemctl_status
      failed_when: "'RESTful Avro schema registry for Apache Kafka' not in systemctl_status.stdout_lines[0]"
      changed_when: false

- name: Verify - kafka_rest
  hosts: kafka_rest
  gather_facts: false
  tasks:
    - shell: grep "apache.kafka.sasl.oauthbearer.allowed.urls=https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/token,https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/certs" /etc/systemd/system/confluent-kafka-rest.service.d/override.conf
      changed_when: false

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka-rest/kafka-rest.properties
        property: client.security.protocol
        expected_value: SASL_SSL

    - name: Check Updated Service Description
      shell: systemctl status confluent-kafka-rest
      register: systemctl_status
      failed_when: "'Custom Rest Proxy description' not in systemctl_status.stdout_lines[0]"
      changed_when: false

- name: Verify - kafka_connect
  hosts: kafka_connect
  gather_facts: false
  tasks:
    - shell: grep "apache.kafka.sasl.oauthbearer.allowed.urls=https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/token,https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/certs" /etc/systemd/system/confluent-kafka-connect.service.d/override.conf
      changed_when: false

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka/connect-distributed.properties
        property: security.protocol
        expected_value: SASL_SSL

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka/connect-distributed.properties
        property: producer.sasl.jaas.config
        expected_value: 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId="kafka_connect" clientSecret="my-secret" ssl.truststore.location="/var/ssl/private/kafka_connect.truststore.jks" ssl.truststore.password="confluenttruststorepass";'

    - name: Check Updated Service Description
      shell: systemctl status confluent-kafka-connect
      register: systemctl_status
      failed_when: "'Custom Connect description' not in systemctl_status.stdout_lines[0]"
      changed_when: false

    - import_role:
        name: confluent.platform.common
        tasks_from: get_authorization_token.yml
      vars:
        oauth: "{{ kafka_connect_oauth_enabled }}"
        oauth_user: "{{ kafka_connect_oauth_user }}"
        oauth_password: "{{ kafka_connect_oauth_password }}"
        ldap_user: "{{ kafka_connect_ldap_user }}"
        ldap_password: "{{ kafka_connect_ldap_password }}"
        cert_auth_only_enabled: false
        oauth_client_assertion_enabled: "{{ kafka_connect_oauth_client_assertion_enabled }}"
        oauth_client_assertion_issuer: "{{ kafka_connect_oauth_client_assertion_issuer }}"
        oauth_client_assertion_sub: "{{ kafka_connect_oauth_client_assertion_sub }}"
        oauth_client_assertion_audience: "{{ kafka_connect_oauth_client_assertion_audience }}"
        oauth_client_assertion_private_key_file: "{{ kafka_connect_oauth_client_assertion_private_key_file_dest_path }}"
        oauth_client_assertion_private_key_passphrase: "{{ kafka_connect_oauth_client_assertion_private_key_passphrase }}"
        oauth_client_assertion_file: "{{ kafka_connect_oauth_client_assertion_file_dest_path }}"
        oauth_client_assertion_template_file: "{{ kafka_connect_oauth_client_assertion_template_file_dest_path }}"

    - name: Get Connectors on connect cluster1
      uri:
        url: "https://kafka-connect1:8083/connectors"
        status_code: 200
        validate_certs: false
        headers:
          Authorization: "Bearer {{ authorization_token }}"
      register: connectors

    - name: Assert Connector Created
      assert:
        that:
          - connectors.json[0] == "sample-connector-1"
        fail_msg: "Connector not created"
        quiet: true

    - import_role:
        name: variables

    - name: Test data
      set_fact:
        kafka_connect_url: "{{kafka_connect_http_protocol}}://{{ hostvars[inventory_hostname]|confluent.platform.resolve_hostname }}:{{kafka_connect_rest_port}}/connectors"
        reset_all_connectors: []

        add_new_connector:
          - name: test-connector-1
            config:
              connector.class: "FileStreamSinkConnector"
              tasks.max: "1"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "test_topic"

        update_existing_connector:
          - name: test-connector-1
            config:
              connector.class: "FileStreamSinkConnector"
              tasks.max: "5"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "test_topic"

        add_bad_request_connector:
          - name: test-connector-1
            config:
              connector.class: "FileStreamSinkConnector"
              tasks.max: "5"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "test_topic"
          - name: test-connector-2
            config:
              connector.class: "org.apache.kafka.connect.mirror.MirrorHeartbeatConnector"
              tasks.max: "1"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "foo"

        add_non_existent_connector:
          - name: test-connector-1
            config:
              connector.class: "FileStreamSinkConnector"
              tasks.max: "5"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "test_topic"
          - name: test-connector-2
            config:
              connector.class: "io.confluent.connect.jdbc.JdbcSinkConnector"
              tasks.max: "1"
              topics: "foo"

        delete_connector:
          - name: test-connector-2
            config:
              connector.class: "FileStreamSinkConnector"
              tasks.max: "1"
              topics: "bar"

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ reset_all_connectors }}"
        expected_message: ""
        token: "{{ authorization_token }}"

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ add_new_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: new connector added."
        token: "{{ authorization_token }}"

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ update_existing_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: connector configuration updated."
        token: "{{ authorization_token }}"

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ add_bad_request_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: no configuration change, test-connector-2: ERROR error while adding new connector configuration (HTTP Error 400: Bad Request)."
        token: "{{ authorization_token }}"

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ add_non_existent_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: no configuration change, test-connector-2: ERROR error while adding new connector configuration (HTTP Error 500: Server Error)."
        token: "{{ authorization_token }}"

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ delete_connector }}"
        expected_message: "Connectors removed: test-connector-1. Connectors added or updated: test-connector-2: new connector added."
        token: "{{ authorization_token }}"

    - name: Get stats on installed remote plugin
      stat:
        path: /usr/share/java/connect_plugins/confluentinc-kafka-connect-s3
      register: installed_remote_plugin

    - name: Assert remote plugin is installed correctly
      assert:
        that:
          - installed_remote_plugin.stat.exists
          - installed_remote_plugin.stat.gr_name == 'confluent'
          - installed_remote_plugin.stat.pw_name == 'cp-kafka-connect'
        quiet: true

    - name: Grab installed remote plugin manifest file
      shell: cat /usr/share/java/connect_plugins/confluentinc-kafka-connect-s3/manifest.json
      register: result
    - name: Parse manifest file
      set_fact:
        jsondata: "{{ result.stdout | from_json }}"
    - name: Assert installed remote plugin version
      assert:
        that:
          - jsondata.version == connect_s3_plugin_version

    - name: Get stats on installed local plugin
      stat:
        path: /usr/share/java/connect_plugins/confluentinc-kafka-connect-gcp-functions
      register: installed_local_plugin

    - name: Assert local plugin is installed correctly
      assert:
        that:
          - installed_local_plugin.stat.exists
          - installed_local_plugin.stat.gr_name == 'confluent'
          - installed_local_plugin.stat.pw_name == 'cp-kafka-connect'
        quiet: true

    - name: Grab installed local plugin manifest file
      shell: cat /usr/share/java/connect_plugins/confluentinc-kafka-connect-gcp-functions/manifest.json
      register: result
    - name: Parse manifest file
      set_fact:
        jsondata: "{{ result.stdout | from_json }}"
    - name: Assert installed local plugin version
      assert:
        that:
          - jsondata.version == "1.1.9"
- name: Verify - ksql
  hosts: ksql
  gather_facts: false
  tasks:
    - shell: grep "apache.kafka.sasl.oauthbearer.allowed.urls=https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/token,https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/certs" /etc/systemd/system/confluent-ksqldb.service.d/override.conf
      changed_when: false

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/ksqldb/ksql-server.properties
        property: security.protocol
        expected_value: SASL_SSL

    - name: Check Updated Service Description
      shell: systemctl status confluent-ksqldb
      register: systemctl_status
      failed_when: "'Custom KSQLDB description' not in systemctl_status.stdout_lines[0]"
      changed_when: false

- name: Verify - control_center_next_gen
  hosts: control_center_next_gen
  gather_facts: false
  tasks:
    - shell: grep "apache.kafka.sasl.oauthbearer.allowed.urls=https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/token,https://oauth1:8443/realms/cp-ansible-realm/protocol/openid-connect/certs" /etc/systemd/system/confluent-control-center.service.d/override.conf
      changed_when: false

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/confluent-control-center/control-center-production.properties
        property: confluent.controlcenter.streams.security.protocol
        expected_value: SASL_SSL

    - name: Check Updated Service Description
      shell: systemctl status confluent-control-center
      register: systemctl_status
      failed_when: "'Custom C3 description' not in systemctl_status.stdout_lines[0]"
      changed_when: false

- name: Verify log4j Configuration
  hosts: all,!kafka_controller_migration,!oauth_server #Workaround to skip the task in migration test, need to remove in CP 8.0
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_log4j.yml

- name: Verify custom JAR preservation
  hosts: kafka_connect
  gather_facts: false
  tasks:
    - name: Check if custom JAR exists inside plugin directory
      stat:
        path: /usr/share/java/connect_plugins/jcustenborder-kafka-connect-spooldir/ibm-mq-connector.jar
      register: custom_jar_check

    - name: Fail if custom JAR is missing
      fail:
        msg: "Custom JAR was overwritten during plugin installation!"
      when: not custom_jar_check.stat.exists

    - name: Success message
      debug:
        msg: "SUCCESS: custom JAR is preserved!"
      when: custom_jar_check.stat.exists
