---
### Validates that Kerberos is enabled across all components.
### Validates that SASL SSL Plaintext is enabled across all components.
### Validates that Connector is running

- name: Verify - kafka_controller
  hosts: kafka_controller
  gather_facts: false
  tasks:
    - import_role:
        name: variables
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/controller/server.properties
        property: controller.quorum.voters
        expected_value: "{{ kafka_controller_quorum_voters }}"
    - import_role:
        name: confluent.test
        tasks_from: check_subproperty.yml
      vars:
        file_path: /etc/controller/server.properties
        property: sasl.enabled.mechanisms
        expected_values:
          - GSSAPI
          - PLAIN
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/controller/server.properties
        property: sasl.kerberos.service.name
        expected_value: kafka-a

- name: Verify - kafka_broker
  hosts: kafka_broker
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_subproperty.yml
      vars:
        file_path: /etc/kafka/server.properties
        property: sasl.enabled.mechanisms
        expected_values:
          - GSSAPI
          - PLAIN

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka/server.properties
        property: sasl.kerberos.service.name
        expected_value: kafka-a

    - name: Copy client properties for plain protocol
      copy:
        src: client_plain.properties
        dest: /etc/kafka/client_plain.properties
        mode: '0644'
      tags: copy

    - name: Create Kafka topic
      shell: kafka-topics --create --topic test-topic \
        --bootstrap-server kafka-broker1:9092 --command-config /etc/kafka/client_plain.properties \
        --replication-factor 1 --partitions 6
      run_once: true
      register: output
      failed_when:
        - "'Topic test-topic already exists' not in output.stdout"
        - "'Created topic test-topic' not in output.stdout"

    - name: Create Topic Data
      shell: |
        seq 10 | kafka-console-producer --topic test-topic \
        --bootstrap-server kafka-broker1:9092 --producer.config /etc/kafka/client_plain.properties
      run_once: true

    - name: Read Topic Data
      shell: kafka-console-consumer --topic test-topic \
        --bootstrap-server kafka-broker1:9092 --timeout-ms 10000 \
        --from-beginning --consumer.config /etc/kafka/client_plain.properties
      run_once: true
      register: consumer_output
      failed_when:
        - "'1\n2\n3\n4\n5\n6\n7\n8\n9\n10' not in consumer_output.stdout"

- name: Verify - schema_registry
  hosts: schema_registry
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/schema-registry/schema-registry.properties
        property: kafkastore.security.protocol
        expected_value: SASL_PLAINTEXT

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/schema-registry/schema-registry.properties
        property: kafkastore.sasl.kerberos.service.name
        expected_value: kafka-a

- name: Verify - kafka_connect
  hosts: kafka_connect
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka/connect-distributed.properties
        property: security.protocol
        expected_value: SASL_PLAINTEXT

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka/connect-distributed.properties
        property: sasl.kerberos.service.name
        expected_value: kafka-a

    - name: Get Connectors on connect cluster1
      uri:
        url: "http://kafka-connect1:8083/connectors"
        status_code: 200
        validate_certs: false
      register: connectors

    - name: Assert Connector Created
      assert:
        that:
          - connectors.json[0] == "sample-connector-1"
        fail_msg: "Connector not created"
        quiet: true

    - import_role:
        name: variables

    - name: Test data
      set_fact:
        kafka_connect_url: "{{kafka_connect_http_protocol}}://{{ hostvars[inventory_hostname]|confluent.platform.resolve_hostname }}:{{kafka_connect_rest_port}}/connectors"
        reset_all_connectors: []

        add_new_connector:
          - name: test-connector-1
            config:
              connector.class: "FileStreamSourceConnector"
              tasks.max: "1"
              file: "/etc/kafka/connect-distributed.properties"
              topic: "test_topic"

        update_existing_connector:
          - name: test-connector-1
            config:
              connector.class: "FileStreamSourceConnector"
              tasks.max: "5"
              file: "/etc/kafka/connect-distributed.properties"
              topic: "test_topic"

        add_bad_request_connector:
          - name: test-connector-1
            config:
              connector.class: "FileStreamSourceConnector"
              tasks.max: "5"
              file: "/etc/kafka/connect-distributed.properties"
              topic: "test_topic"
          - name: test-connector-2
            config:
              connector.class: "org.apache.kafka.connect.mirror.MirrorHeartbeatConnector"
              tasks.max: "1"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "foo"

        add_non_existent_connector:
          - name: test-connector-1
            config:
              connector.class: "FileStreamSourceConnector"
              tasks.max: "5"
              file: "/etc/kafka/connect-distributed.properties"
              topic: "test_topic"
          - name: test-connector-2
            config:
              connector.class: "io.confluent.connect.jdbc.JdbcSinkConnector"
              tasks.max: "1"
              topics: "foo"

        delete_connector:
          - name: test-connector-2
            config:
              connector.class: "FileStreamSourceConnector"
              tasks.max: "1"
              topic: "bar"

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ reset_all_connectors }}"
        expected_message: ""

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ add_new_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: new connector added."

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ update_existing_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: connector configuration updated."

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ add_bad_request_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: no configuration change, test-connector-2: ERROR error while adding new connector configuration (HTTP Error 400: Bad Request)."

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ add_non_existent_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: no configuration change, test-connector-2: ERROR error while adding new connector configuration (HTTP Error 500: Internal Server Error)."

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ delete_connector }}"
        expected_message: "Connectors removed: test-connector-1. Connectors added or updated: test-connector-2: new connector added."

- name: Verify - kafka_rest
  hosts: kafka_rest
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka-rest/kafka-rest.properties
        property: client.security.protocol
        expected_value: SASL_PLAINTEXT

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/kafka-rest/kafka-rest.properties
        property: client.sasl.kerberos.service.name
        expected_value: kafka-a

- name: Verify - ksql
  hosts: ksql
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/ksqldb/ksql-server.properties
        property: security.protocol
        expected_value: SASL_PLAINTEXT

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /etc/ksqldb/ksql-server.properties
        property: sasl.kerberos.service.name
        expected_value: kafka-a

# - name: Verify - control_center
#   hosts: control_center
#   gather_facts: false
#   tasks:
#     - import_role:
#         name: confluent.test
#         tasks_from: check_property.yml
#       vars:
#         file_path: /etc/confluent-control-center/control-center-production.properties
#         property: confluent.controlcenter.streams.security.protocol
#         expected_value: SASL_PLAINTEXT

#     - import_role:
#         name: confluent.test
#         tasks_from: check_property.yml
#       vars:
#         file_path: /etc/confluent-control-center/control-center-production.properties
#         property: confluent.controlcenter.streams.sasl.kerberos.service.name
#         expected_value: kafka-a
