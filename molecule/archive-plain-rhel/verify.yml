---
### Validates that SASL SSL protocol is set across all components.
### Validates that custom log4j configuration is in place.
### Validates that FIPS security is enabled on the Brokers.
### Validates that logredactor is functioning properly for all components as per the rule file.

- name: Verify - kafka_controller
  hosts: kafka_controller
  gather_facts: false
  tasks:
    - import_role:
        name: variables
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/etc/controller/server.properties
        property: controller.quorum.voters
        expected_value: "{{ kafka_controller_quorum_voters }}"
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/etc/controller/server.properties
        property: listener.name.controller.sasl.enabled.mechanisms
        expected_value: PLAIN

- name: Verify - kafka_broker
  hosts: kafka_broker
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/custom-config/kafka-broker/server.properties
        property: listener.name.internal.sasl.enabled.mechanisms
        expected_value: PLAIN

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/custom-config/kafka-broker/server.properties
        property: log.dirs
        expected_value: /tmp/logs1,/tmp/logs2

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/custom-config/kafka-broker/server.properties
        property: security.providers
        expected_value: io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator

    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/custom-config/kafka-broker/client.properties
        property: abc
        expected_value: xyz

- name: Verify - schema_registry
  hosts: schema_registry
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/custom-config/schema-registry/schema-registry.properties
        property: kafkastore.security.protocol
        expected_value: SASL_SSL

- name: Verify - kafka_rest
  hosts: kafka_rest
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/etc/kafka-rest/kafka-rest.properties
        property: client.security.protocol
        expected_value: SASL_SSL

- name: Verify - kafka_connect
  hosts: kafka_connect
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/custom-config/kafka-connect/connect-distributed.properties
        property: security.protocol
        expected_value: SASL_SSL

    - import_role:
        name: variables

    - name: Test data
      set_fact:
        kafka_connect_url: "{{kafka_connect_http_protocol}}://{{ hostvars[inventory_hostname]|confluent.platform.resolve_hostname }}:{{kafka_connect_rest_port}}/connectors"
        reset_all_connectors: []

        add_new_connector:
          - name: test-connector-1
            config:
              connector.class: "org.apache.kafka.connect.tools.VerifiableSinkConnector"
              tasks.max: "1"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "test_topic"

        update_existing_connector:
          - name: test-connector-1
            config:
              connector.class: "org.apache.kafka.connect.tools.VerifiableSinkConnector"
              tasks.max: "5"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "test_topic"

        add_bad_request_connector:
          - name: test-connector-1
            config:
              connector.class: "org.apache.kafka.connect.tools.VerifiableSinkConnector"
              tasks.max: "5"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "test_topic"
          - name: test-connector-2
            config:
              connector.class: "org.apache.kafka.connect.mirror.MirrorHeartbeatConnector"
              tasks.max: "1"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "foo"

        add_non_existent_connector:
          - name: test-connector-1
            config:
              connector.class: "org.apache.kafka.connect.tools.VerifiableSinkConnector"
              tasks.max: "5"
              file: "/etc/kafka/connect-distributed.properties"
              topics: "test_topic"
          - name: test-connector-2
            config:
              connector.class: "io.confluent.connect.s3.S3SinkConnector"
              tasks.max: "1"
              topics: "foo"

        delete_connector:
          - name: test-connector-2
            config:
              connector.class: "org.apache.kafka.connect.tools.VerifiableSinkConnector"
              tasks.max: "1"
              topics: "bar"

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ reset_all_connectors }}"
        expected_message: ""

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ add_new_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: new connector added."

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ update_existing_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: connector configuration updated."

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ add_bad_request_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: no configuration change, test-connector-2: ERROR error while adding new connector configuration (HTTP Error 400: Bad Request)."

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ add_non_existent_connector }}"
        expected_message: "Connectors added or updated: test-connector-1: no configuration change, test-connector-2: ERROR error while adding new connector configuration (HTTP Error 500: Internal Server Error)."

    - import_role:
        name: confluent.test
        tasks_from: check_connector.yml
      vars:
        connect_url_input: "{{ kafka_connect_url }}"
        active_connectors_input: "{{ delete_connector }}"
        expected_message: "Connectors removed: test-connector-1. Connectors added or updated: test-connector-2: new connector added."


- name: Verify - ksql
  hosts: ksql
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/etc/ksqldb/ksql-server.properties
        property: security.protocol
        expected_value: SASL_SSL

- name: Verify - control_center
  hosts: control_center
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_property.yml
      vars:
        file_path: /opt/confluent/etc/confluent-control-center/control-center-production.properties
        property: confluent.controlcenter.streams.security.protocol
        expected_value: SASL_SSL

- name: Verify log4j Configuration
  hosts: all
  gather_facts: false
  tasks:
    - import_role:
        name: confluent.test
        tasks_from: check_log4j.yml

- name: Verify logredactor
  hosts: zookeeper:kafka_controller:kafka_broker:schema_registry:ksql:kafka_rest:control_center
  gather_facts: false
  tasks:
    - import_role:
        name: variables

    - name: Create a mapping of component and log file
      set_fact:
        component_log_file_mapping:
          zookeeper: "{{zookeeper_log_dir}}/zookeeper-server.log"
          controller: "{{kafka_controller_log_dir}}/server.log"
          kafka-broker: "{{kafka_broker_log_dir}}/server.log"
          schema-registry: "{{schema_registry_log_dir}}/schema-registry.log"
          kafka-connect: "{{kafka_connect_log_dir}}/connect.log"
          ksql: "{{ksql_log_dir}}/ksql.log"
          kafka-rest: "{{kafka_rest_log_dir}}/kafka-rest.log"
          control-center: "{{control_center_log_dir}}/control-center.log"

    - name: Parse out the component name
      set_fact:
        component: "{{inventory_hostname[:-1]}}"

    - name: Confirm presence of redacted word in log file
      shell: |
        grep -m 1 "<he-who-must-not-be-named>" {{component_log_file_mapping[component]}}
      register: output
      failed_when: (output.rc == 1) or (output.rc == 2)
